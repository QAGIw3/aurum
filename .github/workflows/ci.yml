name: CI

on:
  push:
    branches: [ main ]
  pull_request:
  schedule:
    - cron: '0 6 * * *'

permissions:
  contents: read
  packages: write
  pages: write
  id-token: write

jobs:
  lint-test:
    name: Lint & Test (Python ${{ matrix.python-version }})
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        python-version: ['3.10', '3.11']
    steps:
      - name: Checkout
        uses: actions/checkout@v4.2.2
      - name: Set up Python
        uses: actions/setup-python@v5.4.0
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .[dev,test]
      - name: Ruff
        run: ruff check .
      - name: Mypy
        run: mypy src
      - name: Pytest
        env:
          PYTHONPATH: src
        run: pytest
      - name: Seatunnel template linter
        env:
          PYTHONPATH: src
        run: python -m aurum.seatunnel.linter
      - name: Kafka schema contract check
        if: matrix.python-version == '3.11'
        env:
          PYTHONPATH: src
        run: python3 scripts/kafka/register_schemas.py --validate-only --include-eia

      - name: Avro schema compatibility check
        if: matrix.python-version == '3.11'
        env:
          PYTHONPATH: src
        run: |
          # Enhanced schema validation
          python3 -c "
          from scripts.kafka.register_schemas import validate_contracts, load_subject_mapping, load_eia_subjects
          from pathlib import Path
          import json

          schema_root = Path('kafka/schemas')
          subjects_file = schema_root / 'subjects.json'

          # Load subjects
          subjects = load_subject_mapping(subjects_file)
          subjects.update(load_eia_subjects(Path('config/eia_ingest_datasets.json')))

          # Validate contracts with strict compatibility
          validated = validate_contracts(subjects, schema_root, 'BACKWARD')

          print(f'✅ Validated {len(validated)} schema contracts with BACKWARD compatibility')

          # Check for potential breaking changes in recent commits
          import subprocess
          result = subprocess.run(['git', 'log', '--oneline', '-n', '10'], capture_output=True, text=True)
          if 'BREAKING' in result.stdout.upper():
              print('⚠️  Recent commits contain BREAKING changes - review schema evolution')
          "
      - name: Schema Registry dry-run
        if: matrix.python-version == '3.11'
        env:
          PYTHONPATH: src
        run: scripts/kafka/bootstrap.sh --schema-registry-url http://localhost:8081 --dry-run

  airflow-dag-check:
    name: Airflow DAG Syntax
    needs: lint-test
    runs-on: ubuntu-latest
    env:
      AIRFLOW_HOME: ${{ runner.temp }}/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: sqlite:///${{ runner.temp }}/airflow/airflow.db
    steps:
      - name: Checkout
        uses: actions/checkout@v4.2.2
      - name: Set up Python
        uses: actions/setup-python@v5.4.0
        with:
          python-version: '3.11'
          cache: 'pip'
      - name: Install Airflow and project dependencies
        run: |
          python -m pip install --upgrade pip
          pip install "apache-airflow==2.8.1" --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-2.8.1/constraints-3.11.txt"
          pip install -e .[dev,test]
      - name: Initialize Airflow metadata DB
        run: airflow db init
      - name: Validate DAG imports
        run: airflow dags list --subdir airflow/dags

  spectral-lint:
    name: Spectral OpenAPI Lint
    needs: lint-test
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4.2.2

      - name: Install Spectral
        run: |
          npm install -g @stoplight/spectral-cli

      - name: Lint OpenAPI specifications
        run: |
          # Lint all OpenAPI files
          spectral lint openapi/**/*.y*ml \
            --ruleset .spectral.yml \
            --format text \
            --fail-severity error \
            --output-format text \
            --verbose

      - name: Validate OpenAPI schema compliance
        run: |
          # Check that all required fields are present
          for file in openapi/**/*.y*ml; do
            echo "Validating $file..."

            # Check for required OpenAPI 3.0 fields
            if ! grep -q "openapi:" "$file"; then
              echo "ERROR: Missing 'openapi' field in $file"
              exit 1
            fi

            if ! grep -q "info:" "$file"; then
              echo "ERROR: Missing 'info' section in $file"
              exit 1
            fi

            if ! grep -q "title:" "$file"; then
              echo "ERROR: Missing 'title' in info section of $file"
              exit 1
            fi

            if ! grep -q "version:" "$file"; then
              echo "ERROR: Missing 'version' in info section of $file"
              exit 1
            fi

            if ! grep -q "paths:" "$file"; then
              echo "ERROR: Missing 'paths' section in $file"
              exit 1
            fi
          done

      - name: Check API consistency
        run: |
          # Ensure all endpoints have consistent response models
          python scripts/ci/validate_openapi_consistency.py

      - name: Validate examples in OpenAPI
        run: |
          # Check that all examples in the OpenAPI spec are valid
          python scripts/ci/validate_openapi_examples.py

  openapi-diff:
    name: OpenAPI Diff
    needs: spectral-lint
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4.2.2

      - name: Fetch main branch
        run: git fetch origin main

      - name: Compare OpenAPI specifications
        run: |
          set -euo pipefail
          mkdir -p .openapi-baseline
          status=0
          for file in $(find openapi -type f \( -name '*.yaml' -o -name '*.yml' \ )); do
            base_name=$(basename "$file")
            if git show origin/main:"$file" > ".openapi-baseline/$base_name"; then
              echo "Comparing $file to origin/main";
              docker run --rm -v "$PWD":/workspace openapitools/openapi-diff:latest \
                /workspace/.openapi-baseline/$base_name /workspace/$file || status=$?
            else
              echo "Skipping $file; not present on main branch"
            fi
          done
          exit $status

  security-scan:
    name: Container Security Scan
    needs: lint-test
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4.2.2
      - name: Prepare SBOM directory
        run: |
          mkdir -p sbom
          mkdir -p security
      - name: Set up Python
        uses: actions/setup-python@v5.4.0
        with:
          python-version: '3.11'
          cache: 'pip'
      - name: Install security tooling
        run: |
          python -m pip install --upgrade pip
          pip install bandit safety
      - name: Hadolint API Dockerfile
        uses: hadolint/hadolint-action@v3.1.0
        with:
          dockerfile: Dockerfile.api
      - name: Hadolint Worker Dockerfile
        uses: hadolint/hadolint-action@v3.1.0
        with:
          dockerfile: Dockerfile.worker
      - name: Bandit SAST
        run: |
          bandit -r src -f sarif -o security/bandit.sarif
      - name: Safety vulnerability audit
        run: |
          safety check --file constraints/dev.txt --json > security/safety.json
        continue-on-error: true

      - name: pip-audit vulnerability scan
        run: |
          pip install pip-audit
          pip-audit --format=json --output-file=security/pip-audit.json || true
        continue-on-error: true

      - name: Upload pip-audit results to Code Scanning
        uses: github/codeql-action/upload-sarif@v3.27.0
        if: always()
        with:
          sarif_file: security/pip-audit.json
        continue-on-error: true

      - name: Syft SBOM generation
        uses: anchore/sbom-action@v0
        with:
          path: .
          format: spdx-json
          output-file: sbom/spdx-python.json
          upload-artifact: true
      - name: Trivy filesystem scan
        uses: aquasecurity/trivy-action@0.13.0
        with:
          scan-type: fs
          security-checks: vuln,config,secret
          ignore-unfixed: false
          severity: HIGH,CRITICAL
          hide-progress: true
          format: sarif
          output: security/trivy.sarif
          exit-code: '1'

      - name: Trivy config scan
        uses: aquasecurity/trivy-action@0.13.0
        with:
          scan-type: config
          security-checks: config
          severity: HIGH,CRITICAL
          format: sarif
          output: security/trivy-config.sarif
          exit-code: '1'

      - name: Trivy secrets scan
        uses: aquasecurity/trivy-action@0.13.0
        with:
          scan-type: fs
          security-checks: secret
          severity: HIGH,CRITICAL
          format: sarif
          output: security/trivy-secrets.sarif
          exit-code: '1'
      - name: Generate SBOM with Syft
        uses: anchore/sbom-action@v0
        with:
          path: .
          format: spdx-json
          output-file: sbom/spdx-syft.json
          upload-artifact: true
      - name: Upload Bandit SARIF to Code Scanning
        uses: github/codeql-action/upload-sarif@v3.27.0
        with:
          sarif_file: security/bandit.sarif
      - name: Upload Trivy SARIF to Code Scanning
        uses: github/codeql-action/upload-sarif@v3.27.0
        with:
          sarif_file: security/trivy.sarif
      - name: Validate SBOM completeness
        run: |
          # Check if SBOM files exist and are valid
          if [ ! -f "sbom/spdx-python.json" ]; then
            echo "❌ SBOM file missing: sbom/spdx-python.json"
            exit 1
          fi

          # Validate SBOM format
          if ! python3 -c "
          import json
          with open('sbom/spdx-python.json', 'r') as f:
              sbom = json.load(f)

          # Check required SPDX fields
          required_fields = ['SPDXID', 'spdxVersion', 'creationInfo', 'name', 'documentNamespace']
          for field in required_fields:
              if field not in sbom:
                  print(f'Missing required field: {field}')
                  exit(1)

          # Check for packages
          if 'packages' not in sbom or len(sbom['packages']) == 0:
              print('No packages found in SBOM')
              exit(1)

          print('✅ SBOM validation passed')
          "; then
            echo "❌ SBOM validation failed"
            exit 1
          fi

      - name: SBOM dependency check
        run: |
          # Ensure SBOM contains expected packages
          python3 -c "
          import json
          with open('sbom/spdx-python.json', 'r') as f:
              sbom = json.load(f)

          # Check for critical dependencies
          critical_packages = ['python', 'pip', 'setuptools']
          found_packages = [pkg['name'] for pkg in sbom.get('packages', [])]

          missing_packages = []
          for pkg in critical_packages:
              if not any(pkg.lower() in name.lower() for name in found_packages):
                  missing_packages.append(pkg)

          if missing_packages:
              print(f'⚠️ Critical packages may be missing from SBOM: {missing_packages}')
              # Don't fail build for this, just warn

          print(f'📦 Found {len(found_packages)} packages in SBOM')
          "

      - name: Check security exceptions
        run: |
          # Check if found vulnerabilities are in the exceptions list
          if [ -f ".security-exceptions.yml" ] && [ -f "security/trivy.sarif" ]; then
            python3 -c "
            import json
            import yaml
            from datetime import datetime

            # Load security exceptions
            with open('.security-exceptions.yml', 'r') as f:
                exceptions = yaml.safe_load(f)

            # Load Trivy results
            with open('security/trivy.sarif', 'r') as f:
                trivy_results = json.load(f)

            # Check if we have vulnerabilities
            vulnerabilities = []
            for run in trivy_results.get('runs', []):
                for result in run.get('results', []):
                    if result.get('kind') == 'fail':
                        for rule in run.get('tool', {}).get('driver', {}).get('rules', []):
                            if rule.get('id') == result.get('ruleId'):
                                vulnerabilities.append({
                                    'id': result.get('ruleId', 'unknown'),
                                    'severity': rule.get('properties', {}).get('security-severity', 'unknown'),
                                    'message': result.get('message', {}).get('text', ''),
                                    'locations': result.get('locations', [])
                                })

            # Check each vulnerability against exceptions
            allowed_vulnerabilities = []
            blocked_vulnerabilities = []

            for vuln in vulnerabilities:
                vuln_id = vuln['id']
                severity = vuln['severity']

                # Check if vulnerability is in exceptions
                if vuln_id in exceptions.get('exceptions', {}):
                    exception = exceptions['exceptions'][vuln_id]
                    exception_severity = exception.get('severity', 'unknown')

                    # Check if exception is still valid
                    expires_str = exception.get('expires', '')
                    if expires_str:
                        try:
                            expires_date = datetime.strptime(expires_str, '%Y-%m-%d')
                            if datetime.now() > expires_date:
                                blocked_vulnerabilities.append(vuln)
                                continue
                        except ValueError:
                            pass  # Invalid date format, treat as expired

                    # Check severity match
                    if severity == exception_severity:
                        allowed_vulnerabilities.append(vuln)
                        print(f'✅ {vuln_id} ({severity}) - allowed by exception')
                    else:
                        blocked_vulnerabilities.append(vuln)
                else:
                    blocked_vulnerabilities.append(vuln)

            # Report results
            print(f'📊 Exception Check Results:')
            print(f'  - Total vulnerabilities: {len(vulnerabilities)}')
            print(f'  - Allowed by exceptions: {len(allowed_vulnerabilities)}')
            print(f'  - Blocked: {len(blocked_vulnerabilities)}')

            if blocked_vulnerabilities:
                print(f'\\n❌ Blocked vulnerabilities:')
                for vuln in blocked_vulnerabilities:
                    print(f'  - {vuln[\"id\"]} ({vuln[\"severity\"]}): {vuln[\"message\"][:100]}...')

                # Check policy limits
                high_blocked = [v for v in blocked_vulnerabilities if v['severity'] == 'HIGH']
                critical_blocked = [v for v in blocked_vulnerabilities if v['severity'] == 'CRITICAL']

                policy = exceptions.get('policy', {})
                max_high = policy.get('max_high_exceptions', 5)
                max_critical = policy.get('max_critical_exceptions', 0)

                if len(critical_blocked) > max_critical:
                    print(f'❌ Policy violation: {len(critical_blocked)} CRITICAL vulnerabilities exceed limit of {max_critical}')
                    exit(1)

                if len(high_blocked) > max_high:
                    print(f'⚠️ Policy warning: {len(high_blocked)} HIGH vulnerabilities exceed recommended limit of {max_high}')

            if not blocked_vulnerabilities:
                print('✅ All vulnerabilities are properly excepted or none found')
            "
          fi

      - name: Security vulnerability summary
        run: |
          # Generate security summary
          echo "🔒 Security Scan Summary"
          echo "========================"

          # Count vulnerabilities by severity
          if [ -f "security/trivy.sarif" ]; then
            HIGH_COUNT=$(grep -o '"HIGH"' security/trivy.sarif | wc -l)
            CRITICAL_COUNT=$(grep -o '"CRITICAL"' security/trivy.sarif | wc -l)

            echo "High severity vulnerabilities: $HIGH_COUNT"
            echo "Critical severity vulnerabilities: $CRITICAL_COUNT"

            if [ "$CRITICAL_COUNT" -gt 0 ]; then
              echo "❌ CRITICAL vulnerabilities found - blocking build"
              echo "📋 See security/trivy.sarif for details"
              exit 1
            elif [ "$HIGH_COUNT" -gt 10 ]; then
              echo "⚠️ High number of HIGH vulnerabilities found"
              echo "Consider addressing these before deployment"
            fi
          fi

          echo "✅ Security scan completed"

      - name: Upload SBOM artifact
        uses: actions/upload-artifact@v4.4.3
        with:
          name: sbom-${{ github.sha }}
          path: sbom/
          retention-days: 90

      - name: Upload security reports
        uses: actions/upload-artifact@v4.4.3
        with:
          name: security-reports-${{ github.sha }}
          path: |
            security/
            sbom/
          retention-days: 90

  scenario-smoke:
    name: Scenario Smoke
    needs: lint-test
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4.2.2

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v4.0.0

      - name: Build API image for smoke test
        run: |
          docker build \
            --file Dockerfile.api \
            --tag aurum-api:local \
            .

      - name: Run scenario smoke test
        env:
          COMPOSE_PROFILES: worker
        run: |
          bash scripts/ci/scenario_smoke.sh

      - name: Run PPA valuation smoke test
        env:
          COMPOSE_PROFILES: ppa-smoke
        run: |
          bash scripts/ci/ppa_smoke.sh

  dbt-build:
    name: dbt Build
    needs: lint-test
    runs-on: ubuntu-latest
    env:
      DBT_PROFILES_DIR: tests/dbt
      DBT_TARGET: duckdb
      DBT_VARS: '{"iceberg_catalog": "main"}'
    steps:
      - name: Checkout
        uses: actions/checkout@v4.2.2

      - name: Set up Python
        uses: actions/setup-python@v5.4.0
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .[dev,test]

      - name: Prepare dbt artifacts directory
        run: |
          mkdir -p artifacts/dbt

      - name: dbt seed
        run: |
          dbt seed --profiles-dir "$DBT_PROFILES_DIR" --target "$DBT_TARGET" --vars "$DBT_VARS" --full-refresh

      - name: dbt compile (stg,int,marts)
        run: |
          dbt compile --profiles-dir "$DBT_PROFILES_DIR" --target "$DBT_TARGET" --vars "$DBT_VARS" -m stg int marts

      - name: dbt build (stg,int,marts)
        run: |
          dbt build --profiles-dir "$DBT_PROFILES_DIR" --target "$DBT_TARGET" --vars "$DBT_VARS" -m stg int marts

      - name: dbt docs generate
        run: |
          dbt docs generate --profiles-dir "$DBT_PROFILES_DIR" --target "$DBT_TARGET" --vars "$DBT_VARS"

      - name: Upload dbt docs
        uses: actions/upload-artifact@v4.4.3
        with:
          name: dbt-docs
          path: artifacts/dbt

  ge-quality:
    name: Great Expectations Quality Gate
    needs: dbt-build
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4.2.2

      - name: Download dbt artifacts
        uses: actions/download-artifact@v4.1.8
        with:
          name: dbt-docs
          path: artifacts/dbt

      - name: Set up Python
        uses: actions/setup-python@v5.4.0
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .[dev,test]

      - name: Run Great Expectations suites
        env:
          PYTHONPATH: src
        run: |
          python scripts/quality/run_ge_checks.py --warehouse artifacts/dbt/warehouse.duckdb

  api-docs:
    name: API Docs
    if: github.ref == 'refs/heads/main'
    needs:
      - lint-test
      - spectral-lint
    runs-on: ubuntu-latest
    env:
      DBT_PROFILES_DIR: tests/dbt
      DBT_TARGET: duckdb
      DBT_VARS: '{"iceberg_catalog": "main"}'
    permissions:
      contents: read
      pages: write
      id-token: write
    steps:
      - name: Checkout
        uses: actions/checkout@v4.2.2

      - name: Set up Python
        uses: actions/setup-python@v5.4.0
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .[dev,test]

      - name: Generate dbt documentation
        run: |
          dbt deps
          dbt docs generate --profiles-dir "$DBT_PROFILES_DIR" --target "$DBT_TARGET" --vars "$DBT_VARS"

      - name: Bundle dbt docs
        run: |
          mkdir -p public/dbt
          cp -R target/* public/dbt/

      - name: Configure Pages
        uses: actions/configure-pages@v5.0.0

      - name: Prepare API documentation bundle
        run: |
          mkdir -p public/api
          cp -R docs/api/. public/api/
          cp openapi/aurum.yaml public/api/aurum.yaml

      - name: Upload GitHub Pages artifact
        uses: actions/upload-pages-artifact@v3.0.2
        with:
          path: public

  build-and-publish:
    name: Build & Publish Images
    needs:
      - lint-test
      - airflow-dag-check
      - spectral-lint
      - security-scan
      - dbt-build
      - scenario-smoke
      - ge-quality
    if: github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4.2.2

      - name: Set up QEMU
        uses: docker/setup-qemu-action@v3.2.0

      - name: Set up Buildx
        uses: docker/setup-buildx-action@v4.0.0

      - name: Log in to GHCR
        uses: docker/login-action@v3.3.0
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Build & push API image
        uses: docker/build-push-action@v6.9.0
        with:
          context: .
          file: Dockerfile.api
          push: true
          tags: |
            ghcr.io/aurum/aurum-api:dev
            ghcr.io/aurum/aurum-api:${{ github.sha }}

      - name: Build & push worker image
        uses: docker/build-push-action@v6.9.0
        with:
          context: .
          file: Dockerfile.worker
          push: true
          tags: |
            ghcr.io/aurum/aurum-worker:dev
            ghcr.io/aurum/aurum-worker:${{ github.sha }}

      - name: Install Helm
        uses: azure/setup-helm@v4.3.0
        with:
          version: v3.13.3

      - name: Build Helm dependencies
        run: |
          helm dependency build k8s/platform/helm

      - name: Package Helm charts
        run: |
          mkdir -p dist/helm-packages
          helm package k8s/api/helm --destination dist/helm-packages
          helm package k8s/scenario-worker/helm --destination dist/helm-packages
          helm package k8s/platform/helm --destination dist/helm-packages

      - name: Push Helm charts to GHCR
        env:
          HELM_REGISTRY: ghcr.io/${{ github.repository_owner }}/helm
          HELM_EXPERIMENTAL_OCI: 1
        run: |
          echo "${{ secrets.GITHUB_TOKEN }}" | helm registry login ghcr.io --username "${{ github.actor }}" --password-stdin
          for chart in dist/helm-packages/*.tgz; do
            helm push "$chart" oci://$HELM_REGISTRY
          done

      - name: Generate Helm repository index
        env:
          REPO_OWNER: ${{ github.repository_owner }}
          REPO_NAME: ${{ github.event.repository.name }}
        run: |
          mkdir -p dist/pages/helm
          cp dist/helm-packages/*.tgz dist/pages/helm/
          helm repo index dist/pages/helm --url https://${REPO_OWNER}.github.io/${REPO_NAME}/helm

      - name: Generate kustomize overlay
        run: |
          mkdir -p dist/kustomize/dev-${GITHUB_SHA}
          cat <<PATCH > dist/kustomize/dev-${GITHUB_SHA}/kustomization.yaml
          apiVersion: kustomize.config.k8s.io/v1beta1
          kind: Kustomization
          namespace: aurum-dev
          resources:
            - ../../k8s/dev
          images:
            - name: ghcr.io/aurum/aurum-api
              newTag: ${GITHUB_SHA}
            - name: ghcr.io/aurum/aurum-worker
              newTag: ${GITHUB_SHA}
          PATCH
      - name: Upload kustomize overlay
        uses: actions/upload-artifact@v4.4.3
        with:
          name: kustomize-images-${{ github.sha }}
          path: dist/kustomize/dev-${{ github.sha }}

      - name: Upload Helm charts
        uses: actions/upload-artifact@v4.4.3
        with:
          name: helm-charts
          path: dist/pages

  deploy-docs:
    name: Deploy Docs
    if: github.ref == 'refs/heads/main'
    needs: api-docs
    runs-on: ubuntu-latest
    permissions:
      pages: write
      id-token: write
    environment:
      name: github-pages
      url: ${{ steps.deploy.outputs.page_url }}
    concurrency:
      group: pages
      cancel-in-progress: false
    steps:
      - name: Deploy to GitHub Pages
        id: deploy
        uses: actions/deploy-pages@v4.0.5

  deploy-helm-pages:
    name: Publish Helm Repo
    needs: build-and-publish
    if: github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest
    permissions:
      contents: read
      pages: write
      id-token: write
    environment:
      name: github-pages
      url: ${{ steps.deploy.outputs.page_url }}
    steps:
      - name: Configure Pages
        uses: actions/configure-pages@v5.0.0

      - name: Download Helm artifacts
        uses: actions/download-artifact@v4.1.8
        with:
          name: helm-charts
          path: public

      - name: Upload Pages artifact
        uses: actions/upload-pages-artifact@v3.0.2
        with:
          path: public

      - name: Deploy to GitHub Pages
        id: deploy
        uses: actions/deploy-pages@v4.0.5
