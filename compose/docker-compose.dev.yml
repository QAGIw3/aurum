x-env-file: &env_file ../.env

x-ppa-smoke-defaults: &ppa_smoke_defaults
  PPA_SMOKE_TENANT_ID: 00000000-0000-0000-0000-000000000001
  PPA_SMOKE_TENANT_NAME: PPA Smoke Tenant
  PPA_SMOKE_SCENARIO_ID: 11111111-1111-1111-1111-111111111111
  PPA_SMOKE_SCENARIO_NAME: PPA Smoke Scenario
  PPA_SMOKE_CONTRACT_ID: 22222222-2222-2222-2222-222222222222
  PPA_SMOKE_CURVE_KEY: PPA.SMOKE.CURVE
  PPA_SMOKE_RUN_ID: ppa-smoke-run
  PPA_SMOKE_ASOF_DATE: 2024-01-01
  PPA_SMOKE_CONTRACT_MONTH: 2024-02-01
  PPA_SMOKE_SCENARIO_VALUE: "72.5"
  PPA_SMOKE_PPA_PRICE: "50.0"
  PPA_SMOKE_VOLUME_MWH: "10.0"
  PPA_SMOKE_UPFRONT_COST: "100.0"

services:
  minio:
    image: minio/minio:RELEASE.2024-01-18T22-51-28Z
    env_file: *env_file
    command: server /data --console-address ":9001"
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: ${AURUM_S3_ACCESS_KEY}
      MINIO_ROOT_PASSWORD: ${AURUM_S3_SECRET_KEY}
    volumes:
      - minio-data:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/ready"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - aurum

  postgres:
    image: postgres:15
    env_file: *env_file
    environment:
      POSTGRES_DB: aurum
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    ports:
      - "5432:5432"
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ../postgres/ddl:/docker-entrypoint-initdb.d
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "${POSTGRES_USER}"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - aurum

  timescale:
    image: timescale/timescaledb:latest-pg15
    env_file: *env_file
    environment:
      POSTGRES_DB: timeseries
      POSTGRES_USER: ${TIMESCALE_USER}
      POSTGRES_PASSWORD: ${TIMESCALE_PASSWORD}
    ports:
      - "5433:5432"
    volumes:
      - timescale-data:/var/lib/postgresql/data
      - ../timescale:/docker-entrypoint-initdb.d
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "${TIMESCALE_USER}"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - aurum

  redis:
    image: redis:7-alpine
    env_file: *env_file
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - aurum

  zookeeper:
    image: bitnami/zookeeper:3.7
    env_file: *env_file
    environment:
      ALLOW_ANONYMOUS_LOGIN: "yes"
    ports:
      - "2181:2181"
    healthcheck:
      test: ["CMD", "/opt/bitnami/scripts/zookeeper/healthcheck.sh"]
      interval: 10s
      timeout: 10s
      retries: 6
    networks:
      - aurum

  kafka:
    image: bitnami/kafka:3.6
    env_file: *env_file
    depends_on:
      zookeeper:
        condition: service_healthy
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_CFG_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_CFG_LISTENERS: PLAINTEXT://:9092,PLAINTEXT_HOST://:29092
      KAFKA_CFG_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE: "true"
      ALLOW_PLAINTEXT_LISTENER: "yes"
    ports:
      - "29092:29092"
    volumes:
      - kafka-data:/bitnami/kafka
    healthcheck:
      test: ["CMD", "bash", "-c", "/opt/bitnami/kafka/bin/kafka-topics.sh --bootstrap-server localhost:9092 --list >/dev/null"]
      interval: 10s
      timeout: 10s
      retries: 6
    networks:
      - aurum

  schema-registry:
    image: confluentinc/cp-schema-registry:7.5.1
    env_file: *env_file
    depends_on:
      kafka:
        condition: service_healthy
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: PLAINTEXT://kafka:9092
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081
    ports:
      - "8081:8081"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081/subjects"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - aurum

  clickhouse:
    image: clickhouse/clickhouse-server:23.8
    env_file: *env_file
    ports:
      - "8123:8123"
      - "9009:9009"
    volumes:
      - clickhouse-data:/var/lib/clickhouse
      - ../clickhouse:/docker-entrypoint-initdb.d
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8123/ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - aurum

  nessie:
    image: ghcr.io/projectnessie/nessie:0.71.0
    env_file: *env_file
    environment:
      NESSIE_VERSION_STORE_TYPE: INMEMORY
    ports:
      - "19120:19120"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:19120/api/v1/config"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - aurum

  lakefs:
    image: treeverse/lakefs:1.13.0
    env_file: *env_file
    depends_on:
      minio:
        condition: service_healthy
      postgres:
        condition: service_healthy
    environment:
      LAKEFS_AUTH_ENCRYPT_SECRET_KEY: ${LAKEFS_SECRET_KEY}
      LAKEFS_DATABASE_TYPE: postgresql
      LAKEFS_DATABASE_URI: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/aurum?sslmode=disable
      LAKEFS_BLOCKSTORE_TYPE: s3
      LAKEFS_BLOCKSTORE_S3_CREDENTIALS_ACCESS_KEY_ID: ${AURUM_S3_ACCESS_KEY}
      LAKEFS_BLOCKSTORE_S3_CREDENTIALS_SECRET_ACCESS_KEY: ${AURUM_S3_SECRET_KEY}
      LAKEFS_BLOCKSTORE_S3_REGION: us-east-1
      LAKEFS_BLOCKSTORE_S3_ENDPOINT: http://minio:9000
      LAKEFS_GATEWAYS_S3_DOMAIN_NAME: localhost
      LAKEFS_LOGGING_LEVEL: INFO
      LAKEFS_ROOT_USER: ${LAKEFS_ROOT_USER}
      LAKEFS_ROOT_PASSWORD: ${LAKEFS_ROOT_PASSWORD}
    ports:
      - "8000:8000"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/_health"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - aurum

  trino:
    image: trinodb/trino:430
    env_file: *env_file
    depends_on:
      nessie:
        condition: service_healthy
      lakefs:
        condition: service_healthy
      postgres:
        condition: service_healthy
      clickhouse:
        condition: service_healthy
    environment:
      JAVA_TOOL_OPTIONS: "-Xms512M -Xmx2G"
    ports:
      - "8080:8080"
    volumes:
      - ../trino/catalog:/etc/trino/catalog
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/v1/info"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - aurum

  api:
    image: python:3.11-slim
    env_file: *env_file
    depends_on:
      trino:
        condition: service_healthy
      redis:
        condition: service_healthy
    working_dir: /workspace
    volumes:
      - ..:/workspace
    command: >-
      bash -lc "pip install --no-cache-dir -e .[api] && uvicorn aurum.api.app:app --host 0.0.0.0 --port 8085"
    environment:
      AURUM_API_TRINO_HOST: trino
      AURUM_API_TRINO_PORT: 8080
      AURUM_API_TRINO_USER: aurum
      AURUM_API_TRINO_SCHEME: http
      AURUM_APP_DB_DSN: postgresql://${POSTGRES_USER:-aurum}:${POSTGRES_PASSWORD:-aurum}@postgres:5432/aurum
      AURUM_API_REDIS_URL: redis://redis:6379/0
      AURUM_API_CACHE_TTL: 60
      AURUM_API_INMEMORY_TTL: 60
      AURUM_API_SCENARIO_OUTPUTS_ENABLED: "1"
      AURUM_API_RATE_LIMIT_RPS: 10
      AURUM_API_RATE_LIMIT_BURST: 20
      AURUM_API_GZIP_MIN_BYTES: 500
      AURUM_API_AUTH_DISABLED: "${AURUM_API_AUTH_DISABLED:-1}"
      AURUM_API_ADMIN_GROUP: ${AURUM_API_ADMIN_GROUP:-aurum-admins}
      AURUM_API_CORS_ORIGINS: "${AURUM_API_CORS_ORIGINS:-*}"
      AURUM_OTEL_SERVICE_NAME: aurum-api
      AURUM_OTEL_EXPORTER_ENDPOINT: '${AURUM_OTEL_EXPORTER_ENDPOINT:-}'
      AURUM_OTEL_EXPORTER_INSECURE: "${AURUM_OTEL_EXPORTER_INSECURE:-true}"
      <<: *ppa_smoke_defaults
    ports:
      - "8095:8085"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8085/health"]
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - aurum

  ppa-smoke-seed:
    profiles: ["ppa-smoke"]
    image: python:3.11-slim
    env_file: *env_file
    depends_on:
      postgres:
        condition: service_healthy
      trino:
        condition: service_healthy
    working_dir: /workspace
    volumes:
      - ..:/workspace
    environment:
      <<: *ppa_smoke_defaults
    command: >-
      bash -lc "pip install --no-cache-dir psycopg[binary]==3.2.1 trino==0.328.0 && python - <<'PY'
import json
import os

import psycopg
from trino.dbapi import connect


def _dec(value: float) -> str:
    return f"{value:.6f}"


TENANT_ID = os.environ["PPA_SMOKE_TENANT_ID"]
TENANT_NAME = os.environ.get("PPA_SMOKE_TENANT_NAME", "PPA Smoke Tenant")
SCENARIO_ID = os.environ["PPA_SMOKE_SCENARIO_ID"]
SCENARIO_NAME = os.environ.get("PPA_SMOKE_SCENARIO_NAME", "PPA Smoke Scenario")
CONTRACT_ID = os.environ["PPA_SMOKE_CONTRACT_ID"]
CURVE_KEY = os.environ.get("PPA_SMOKE_CURVE_KEY", "PPA.SMOKE.CURVE")
RUN_ID = os.environ.get("PPA_SMOKE_RUN_ID", "ppa-smoke-run")
ASOF_DATE = os.environ.get("PPA_SMOKE_ASOF_DATE", "2024-01-01")
CONTRACT_MONTH = os.environ.get("PPA_SMOKE_CONTRACT_MONTH", "2024-02-01")
SCENARIO_VALUE = float(os.environ.get("PPA_SMOKE_SCENARIO_VALUE", "72.5"))
PPA_PRICE = float(os.environ.get("PPA_SMOKE_PPA_PRICE", "50.0"))
VOLUME = float(os.environ.get("PPA_SMOKE_VOLUME_MWH", "10.0"))
UPFRONT_COST = float(os.environ.get("PPA_SMOKE_UPFRONT_COST", "100.0"))
VERSION_HASH = "ppa-smoke-v1"
dsn = os.environ.get("AURUM_APP_DB_DSN", "postgresql://aurum:aurum@postgres:5432/aurum")
with psycopg.connect(dsn) as conn:
    with conn.cursor() as cur:
        cur.execute(
            """
            INSERT INTO tenant (id, name)
            VALUES (%s, %s)
            ON CONFLICT (id) DO UPDATE SET name = EXCLUDED.name
            """,
            (TENANT_ID, TENANT_NAME),
        )
        cur.execute(
            """
            INSERT INTO scenario (id, tenant_id, name, description, created_by)
            VALUES (%s, %s, %s, %s, %s)
            ON CONFLICT (id) DO UPDATE
            SET name = EXCLUDED.name,
                description = EXCLUDED.description,
                updated_at = NOW()
            """,
            (
                SCENARIO_ID,
                TENANT_ID,
                SCENARIO_NAME,
                "Seeded for PPA smoke test",
                "ppa-smoke",
            ),
        )
        cur.execute(
            """
            INSERT INTO ppa_contract (id, tenant_id, instrument_id, terms)
            VALUES (%s, %s, %s, %s::jsonb)
            ON CONFLICT (id) DO UPDATE
            SET instrument_id = EXCLUDED.instrument_id,
                terms = EXCLUDED.terms,
                updated_at = NOW()
            """,
            (
                CONTRACT_ID,
                TENANT_ID,
                None,
                json.dumps({"ppa_price": PPA_PRICE, "volume_mwh": VOLUME}),
            ),
        )

ddl_statements = [
    "CREATE SCHEMA IF NOT EXISTS iceberg.market",
    """
    CREATE TABLE IF NOT EXISTS iceberg.market.scenario_output (
        asof_date DATE,
        scenario_id VARCHAR,
        tenant_id VARCHAR,
        run_id VARCHAR,
        curve_key VARCHAR,
        tenor_type VARCHAR,
        contract_month DATE,
        tenor_label VARCHAR,
        metric VARCHAR,
        value DOUBLE,
        band_lower DOUBLE,
        band_upper DOUBLE,
        attribution VARCHAR,
        version_hash VARCHAR,
        computed_ts TIMESTAMP(6),
        _ingest_ts TIMESTAMP(6)
    )
    WITH (
        format = 'PARQUET',
        partitioning = ARRAY['scenario_id', 'metric', 'year(asof_date)']
    )
    """,
    """
    CREATE OR REPLACE VIEW iceberg.market.scenario_output_latest AS
    WITH ranked AS (
        SELECT
            asof_date,
            scenario_id,
            tenant_id,
            run_id,
            curve_key,
            tenor_type,
            contract_month,
            tenor_label,
            metric,
            value,
            band_lower,
            band_upper,
            attribution,
            version_hash,
            computed_ts,
            _ingest_ts,
            row_number() OVER (
                PARTITION BY tenant_id, scenario_id, curve_key, metric, tenor_label
                ORDER BY asof_date DESC, computed_ts DESC NULLS LAST, _ingest_ts DESC NULLS LAST
            ) AS rn
        FROM iceberg.market.scenario_output
    )
    SELECT
        asof_date,
        scenario_id,
        tenant_id,
        run_id,
        curve_key,
        tenor_type,
        contract_month,
        tenor_label,
        metric,
        value,
        band_lower,
        band_upper,
        attribution,
        version_hash,
        computed_ts,
        _ingest_ts
    FROM ranked
    WHERE rn = 1
    """,
    """
    CREATE TABLE IF NOT EXISTS iceberg.market.ppa_valuation (
        asof_date DATE,
        ppa_contract_id VARCHAR,
        scenario_id VARCHAR,
        tenant_id VARCHAR,
        curve_key VARCHAR,
        period_start DATE,
        period_end DATE,
        cashflow DECIMAL(18, 6),
        npv DECIMAL(18, 6),
        irr DOUBLE,
        metric VARCHAR,
        value DECIMAL(18, 6),
        version_hash VARCHAR,
        _ingest_ts TIMESTAMP(6)
    )
    WITH (
        format = 'PARQUET',
        partitioning = ARRAY['tenant_id', 'ppa_contract_id', 'year(asof_date)']
    )
    """,
]

cashflow_value = (SCENARIO_VALUE - PPA_PRICE) * VOLUME
npv_value = cashflow_value - UPFRONT_COST
irr_value = cashflow_value / UPFRONT_COST - 1.0 if UPFRONT_COST else 0.0

with connect(host="trino", port=8080, user="aurum", http_scheme="http") as trino_conn:
    cur = trino_conn.cursor()
    for stmt in ddl_statements:
        cur.execute(stmt)

    cur.execute(
        f"DELETE FROM iceberg.market.scenario_output WHERE scenario_id = '{SCENARIO_ID}'"
    )
    cur.execute(
        f"""
        INSERT INTO iceberg.market.scenario_output (
            asof_date,
            scenario_id,
            tenant_id,
            run_id,
            curve_key,
            tenor_type,
            contract_month,
            tenor_label,
            metric,
            value,
            band_lower,
            band_upper,
            attribution,
            version_hash,
            computed_ts,
            _ingest_ts
        ) VALUES (
            DATE '{ASOF_DATE}',
            '{SCENARIO_ID}',
            '{TENANT_ID}',
            '{RUN_ID}',
            '{CURVE_KEY}',
            'MONTHLY',
            DATE '{CONTRACT_MONTH}',
            '{CONTRACT_MONTH}',
            'mid',
            {SCENARIO_VALUE},
            NULL,
            NULL,
            NULL,
            '{VERSION_HASH}',
            TIMESTAMP '{ASOF_DATE} 00:00:00',
            CURRENT_TIMESTAMP
        )
        """
    )

    cur.execute(
        f"DELETE FROM iceberg.market.ppa_valuation WHERE ppa_contract_id = '{CONTRACT_ID}' AND scenario_id = '{SCENARIO_ID}'"
    )
    cur.execute(
        f"""
        INSERT INTO iceberg.market.ppa_valuation (
            asof_date,
            ppa_contract_id,
            scenario_id,
            tenant_id,
            curve_key,
            period_start,
            period_end,
            cashflow,
            npv,
            irr,
            metric,
            value,
            version_hash,
            _ingest_ts
        ) VALUES (
            DATE '{ASOF_DATE}',
            '{CONTRACT_ID}',
            '{SCENARIO_ID}',
            '{TENANT_ID}',
            '{CURVE_KEY}',
            DATE '{CONTRACT_MONTH}',
            DATE '{CONTRACT_MONTH}',
            DECIMAL '{_dec(cashflow_value)}',
            NULL,
            NULL,
            'cashflow',
            DECIMAL '{_dec(cashflow_value)}',
            '{VERSION_HASH}',
            CURRENT_TIMESTAMP
        )
        """
    )
    cur.execute(
        f"""
        INSERT INTO iceberg.market.ppa_valuation (
            asof_date,
            ppa_contract_id,
            scenario_id,
            tenant_id,
            curve_key,
            period_start,
            period_end,
            cashflow,
            npv,
            irr,
            metric,
            value,
            version_hash,
            _ingest_ts
        ) VALUES (
            DATE '{ASOF_DATE}',
            '{CONTRACT_ID}',
            '{SCENARIO_ID}',
            '{TENANT_ID}',
            '{CURVE_KEY}',
            DATE '{CONTRACT_MONTH}',
            DATE '{CONTRACT_MONTH}',
            NULL,
            DECIMAL '{_dec(npv_value)}',
            NULL,
            'NPV',
            DECIMAL '{_dec(npv_value)}',
            '{VERSION_HASH}',
            CURRENT_TIMESTAMP
        )
        """
    )
    cur.execute(
        f"""
        INSERT INTO iceberg.market.ppa_valuation (
            asof_date,
            ppa_contract_id,
            scenario_id,
            tenant_id,
            curve_key,
            period_start,
            period_end,
            cashflow,
            npv,
            irr,
            metric,
            value,
            version_hash,
            _ingest_ts
        ) VALUES (
            DATE '{ASOF_DATE}',
            '{CONTRACT_ID}',
            '{SCENARIO_ID}',
            '{TENANT_ID}',
            '{CURVE_KEY}',
            DATE '{CONTRACT_MONTH}',
            DATE '{CONTRACT_MONTH}',
            NULL,
            NULL,
            {irr_value},
            'IRR',
            DECIMAL '{_dec(irr_value)}',
            '{VERSION_HASH}',
            CURRENT_TIMESTAMP
        )
        """
    )

print("Seeded PPA smoke data.")
PY"
    networks:
      - aurum

  api-built:
    profiles: ["api-built", "pipeline"]
    image: ${AURUM_API_IMAGE:-aurum-api:local}
    build:
      context: ..
      dockerfile: Dockerfile.api
    env_file: *env_file
    depends_on:
      trino:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      AURUM_API_TRINO_HOST: trino
      AURUM_API_TRINO_PORT: 8080
      AURUM_API_TRINO_USER: aurum
      AURUM_API_TRINO_SCHEME: http
      AURUM_APP_DB_DSN: postgresql://${POSTGRES_USER:-aurum}:${POSTGRES_PASSWORD:-aurum}@postgres:5432/aurum
      AURUM_API_REDIS_URL: redis://redis:6379/0
      AURUM_API_CACHE_TTL: 60
      AURUM_API_INMEMORY_TTL: 60
      AURUM_API_SCENARIO_OUTPUTS_ENABLED: "1"
      AURUM_API_RATE_LIMIT_RPS: 10
      AURUM_API_RATE_LIMIT_BURST: 20
      AURUM_API_GZIP_MIN_BYTES: 500
      AURUM_API_AUTH_DISABLED: "${AURUM_API_AUTH_DISABLED:-1}"
      AURUM_API_ADMIN_GROUP: ${AURUM_API_ADMIN_GROUP:-aurum-admins}
      AURUM_API_CORS_ORIGINS: "${AURUM_API_CORS_ORIGINS:-*}"
      AURUM_OTEL_SERVICE_NAME: aurum-api
      AURUM_OTEL_EXPORTER_ENDPOINT: '${AURUM_OTEL_EXPORTER_ENDPOINT:-}'
      AURUM_OTEL_EXPORTER_INSECURE: "${AURUM_OTEL_EXPORTER_INSECURE:-true}"
    ports:
      - "8096:8080"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - aurum

  oauth2-proxy:
    profiles: ["auth"]
    image: quay.io/oauth2-proxy/oauth2-proxy:v7.5.1
    env_file: *env_file
    depends_on:
      api:
        condition: service_started
    environment:
      OAUTH2_PROXY_PROVIDER: ${OAUTH2_PROXY_PROVIDER:-oidc}
      OAUTH2_PROXY_OIDC_ISSUER_URL: ${OAUTH2_PROXY_ISSUER_URL}
      OAUTH2_PROXY_CLIENT_ID: ${OAUTH2_PROXY_CLIENT_ID}
      OAUTH2_PROXY_CLIENT_SECRET: ${OAUTH2_PROXY_CLIENT_SECRET}
      OAUTH2_PROXY_REDIRECT_URL: ${OAUTH2_PROXY_REDIRECT_URL:-http://api.aurum.localhost/oauth2/callback}
      OAUTH2_PROXY_COOKIE_SECRET: ${OAUTH2_PROXY_COOKIE_SECRET}
      OAUTH2_PROXY_EMAIL_DOMAINS: ${OAUTH2_PROXY_EMAIL_DOMAINS:-*}
      OAUTH2_PROXY_HTTP_ADDRESS: 0.0.0.0:4180
      OAUTH2_PROXY_UPSTREAMS: file:///dev/null
      OAUTH2_PROXY_COOKIE_SECURE: "false"
      OAUTH2_PROXY_COOKIE_SAMESITE: lax
      OAUTH2_PROXY_SKIP_PROVIDER_BUTTON: "true"
      OAUTH2_PROXY_SET_AUTHORIZATION_HEADER: "true"
      OAUTH2_PROXY_SET_XAUTHREQUEST: "true"
    networks:
      - aurum

  traefik:
    profiles: ["auth"]
    image: traefik:v2.11
    depends_on:
      oauth2-proxy:
        condition: service_started
    command:
      - --providers.file.directory=/etc/traefik/dynamic
      - --entrypoints.web.address=:8082
    volumes:
      - ./traefik/traefik.yml:/etc/traefik/traefik.yml:ro
      - ./traefik/dynamic:/etc/traefik/dynamic:ro
    ports:
      - "8082:8082"
    networks:
      - aurum

  scenario-worker:
    profiles: ["worker", "pipeline"]
    image: python:3.11-slim
    env_file: *env_file
    depends_on:
      kafka:
        condition: service_healthy
      schema-registry:
        condition: service_healthy
    working_dir: /workspace
    volumes:
      - ..:/workspace
    command: >-
      bash -lc "pip install --no-cache-dir -e .[api,worker] && python -m aurum.scenarios.worker"
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      SCHEMA_REGISTRY_URL: http://schema-registry:8081
      AURUM_SCENARIO_REQUEST_TOPIC: aurum.scenario.request.v1
      AURUM_SCENARIO_OUTPUT_TOPIC: aurum.scenario.output.v1
      AURUM_SCENARIO_WORKER_GROUP: aurum-scenario-worker
      AURUM_SCENARIO_METRICS_PORT: 9464
      AURUM_SCENARIO_METRICS_ADDR: 0.0.0.0
      AURUM_SCENARIO_MAX_ATTEMPTS: 3
      AURUM_SCENARIO_RETRY_BACKOFF_SEC: 0.5
      AURUM_OTEL_SERVICE_NAME: aurum-scenario-worker
      AURUM_OTEL_EXPORTER_ENDPOINT: '${AURUM_OTEL_EXPORTER_ENDPOINT:-}'
      AURUM_OTEL_EXPORTER_INSECURE: "${AURUM_OTEL_EXPORTER_INSECURE:-true}"
    healthcheck:
      test:
        [
          "CMD",
          "python",
          "-c",
          "import os, urllib.request; port = os.getenv('AURUM_SCENARIO_HTTP_PORT') or os.getenv('AURUM_SCENARIO_METRICS_PORT', '9464'); urllib.request.urlopen('http://127.0.0.1:%s/ready' % port)",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - aurum

  scenario-worker-built:
    profiles: ["worker-built", "pipeline"]
    image: ${AURUM_WORKER_IMAGE:-aurum-worker:local}
    build:
      context: ..
      dockerfile: Dockerfile.worker
    env_file: *env_file
    depends_on:
      kafka:
        condition: service_healthy
      schema-registry:
        condition: service_healthy
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      SCHEMA_REGISTRY_URL: http://schema-registry:8081
      AURUM_SCENARIO_REQUEST_TOPIC: aurum.scenario.request.v1
      AURUM_SCENARIO_OUTPUT_TOPIC: aurum.scenario.output.v1
      AURUM_SCENARIO_WORKER_GROUP: aurum-scenario-worker
      AURUM_SCENARIO_METRICS_PORT: 9464
      AURUM_SCENARIO_METRICS_ADDR: 0.0.0.0
      AURUM_SCENARIO_MAX_ATTEMPTS: 3
      AURUM_SCENARIO_RETRY_BACKOFF_SEC: 0.5
      AURUM_OTEL_SERVICE_NAME: aurum-scenario-worker
      AURUM_OTEL_EXPORTER_ENDPOINT: '${AURUM_OTEL_EXPORTER_ENDPOINT:-}'
      AURUM_OTEL_EXPORTER_INSECURE: "${AURUM_OTEL_EXPORTER_INSECURE:-true}"
    healthcheck:
      test:
        [
          "CMD",
          "python",
          "-c",
          "import os, urllib.request; port = os.getenv('AURUM_SCENARIO_HTTP_PORT') or os.getenv('AURUM_SCENARIO_METRICS_PORT', '9464'); urllib.request.urlopen('http://127.0.0.1:%s/ready' % port)",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - aurum

  vector:
    image: timberio/vector:0.33.0-alpine
    env_file: *env_file
    depends_on:
      clickhouse:
        condition: service_healthy
    volumes:
      - ../vector/vector.toml:/etc/vector/vector.toml:ro
      - /var/run/docker.sock:/var/run/docker.sock
    healthcheck:
      test: ["CMD", "vector", "validate", "/etc/vector/vector.toml"]
      interval: 60s
      timeout: 10s
      retries: 3
    networks:
      - aurum

  airflow-init:
    image: apache/airflow:2.8.1
    env_file: *env_file
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${AIRFLOW_DB_USER}:${AIRFLOW_DB_PASSWORD}@postgres:5432/airflow
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY}
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      _AIRFLOW_WWW_USER_USERNAME: ${AIRFLOW_ADMIN_USER}
      _AIRFLOW_WWW_USER_PASSWORD: ${AIRFLOW_ADMIN_PASSWORD}
      AIRFLOW_HOME: /opt/airflow
    command: >-
      bash -c "airflow db init && airflow users create --role Admin --username '${AIRFLOW_ADMIN_USER}' --password '${AIRFLOW_ADMIN_PASSWORD}' --firstname Aurum --lastname Admin --email ${AIRFLOW_ADMIN_EMAIL} || true"
    volumes:
      - airflow-data:/opt/airflow
      - ../airflow/dags:/opt/airflow/dags
      - ../src:/opt/airflow/src
      - ../scripts:/opt/airflow/scripts
      - ../seatunnel:/opt/airflow/seatunnel
    networks:
      - aurum

  airflow-webserver:
    image: apache/airflow:2.8.1
    env_file: *env_file
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${AIRFLOW_DB_USER}:${AIRFLOW_DB_PASSWORD}@postgres:5432/airflow
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY}
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__WEBSERVER__DEFAULT_UI_TIMEZONE: UTC
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "True"
      AIRFLOW_HOME: /opt/airflow
      PYTHONPATH: /opt/airflow/src
    ports:
      - "8088:8080"
    volumes:
      - airflow-data:/opt/airflow
      - ../airflow/dags:/opt/airflow/dags
      - ../src:/opt/airflow/src
      - ../scripts:/opt/airflow/scripts
      - ../seatunnel:/opt/airflow/seatunnel
    command: webserver
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - aurum

  airflow-scheduler:
    image: apache/airflow:2.8.1
    env_file: *env_file
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${AIRFLOW_DB_USER}:${AIRFLOW_DB_PASSWORD}@postgres:5432/airflow
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY}
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW_HOME: /opt/airflow
      PYTHONPATH: /opt/airflow/src
    volumes:
      - airflow-data:/opt/airflow
      - ../airflow/dags:/opt/airflow/dags
      - ../src:/opt/airflow/src
      - ../scripts:/opt/airflow/scripts
      - ../seatunnel:/opt/airflow/seatunnel
    command: scheduler
    healthcheck:
      test: ["CMD", "airflow", "jobs", "check", "--job-type", "SchedulerJob", "--hostname", "$(hostname)"]
      interval: 60s
      timeout: 15s
      retries: 5
    networks:
      - aurum

  superset:
    profiles: ["ui"]
    image: apache/superset:3.0.0
    env_file: *env_file
    depends_on:
      trino:
        condition: service_healthy
    environment:
      SUPERSET_SECRET_KEY: ${SUPERSET_SECRET_KEY}
      SUPERSET_LOAD_EXAMPLES: "no"
      SUPERSET_ENV: production
    command: >-
      /bin/sh -c "superset db upgrade && superset init && superset run -h 0.0.0.0 -p 8088"
    ports:
      - "8089:8088"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8088/health"]
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - aurum

  kafka-ui:
    profiles: ["ui"]
    image: provectuslabs/kafka-ui:master
    env_file: *env_file
    depends_on:
      kafka:
        condition: service_healthy
      schema-registry:
        condition: service_healthy
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092
      KAFKA_CLUSTERS_0_SCHEMAREGISTRY: http://schema-registry:8081
    ports:
      - "8090:8080"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - aurum

  bootstrap:
    profiles: ["bootstrap"]
    image: python:3.11-slim
    env_file: *env_file
    depends_on:
      minio:
        condition: service_healthy
      lakefs:
        condition: service_healthy
    volumes:
      - ../scripts/bootstrap:/bootstrap:ro
    entrypoint: ["/bin/sh", "-c", "pip install --no-cache-dir boto3 requests && python /bootstrap/bootstrap_dev.py"]
    networks:
      - aurum

volumes:
  minio-data:
  postgres-data:
  timescale-data:
  kafka-data:
  clickhouse-data:
  airflow-data:

networks:
  aurum:
    driver: bridge
