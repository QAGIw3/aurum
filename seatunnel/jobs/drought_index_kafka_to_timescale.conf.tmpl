# Drought index topic â†’ Timescale hypertable (public.drought_index_timeseries)
#
# Required environment variables:
#   KAFKA_BOOTSTRAP_SERVERS  - Kafka brokers
#   SCHEMA_REGISTRY_URL      - Schema Registry endpoint
#   TIMESCALE_JDBC_URL       - JDBC URL for TimescaleDB
#   TIMESCALE_USER           - Timescale username
#   TIMESCALE_PASSWORD       - Timescale password
#

env {
  job.mode = "STREAMING"
  checkpoint.interval = 60000
}

source {
  Kafka {
    bootstrap.servers = "${KAFKA_BOOTSTRAP_SERVERS}"
    topic = "aurum.drought.index.v1"
    format = "avro"
    start_mode = "latest"
    avro {
      schema.registry.url = "${SCHEMA_REGISTRY_URL}"
    }
    result_table_name = "drought_index_raw"
  }
}

transform {
  Sql {
    source_table_name = "drought_index_raw"
    result_table_name = "drought_index_timescale"
    query = """
      SELECT
        series_id,
        region_type,
        region_id,
        dataset,
        index,
        timescale,
        (DATE '1970-01-01' + valid_date * INTERVAL '1' DAY) AS valid_time,
        value,
        unit,
        CASE WHEN as_of IS NOT NULL THEN TO_TIMESTAMP(as_of / 1000000.0) ELSE NULL END AS as_of,
        source_url,
        ingest_job_id,
        TO_TIMESTAMP(ingest_ts / 1000000.0) AS ingest_ts,
        CAST(metadata AS STRING) AS metadata
      FROM drought_index_raw
    """
  }
}

sink {
  Jdbc {
    plugin_input = "drought_index_timescale"
    driver = "org.postgresql.Driver"
    url = "${TIMESCALE_JDBC_URL}"
    user = "${TIMESCALE_USER}"
    password = "${TIMESCALE_PASSWORD}"
    table = "public.drought_index_timeseries"
    primary_keys = ["series_id", "valid_time"]
    save_mode = "upsert"
    connection_check_timeout_sec = 30
  }
}

