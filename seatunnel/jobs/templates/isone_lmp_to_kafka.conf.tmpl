# ISO-NE web services LMP â†’ Kafka (Avro)
#
# Required environment variables:
#   ISONE_URL               - ISO-NE web service endpoint returning JSON
#   ISONE_START             - ISO-NE start datetime (YYYY-MM-DDTHH:MM:SSZ)
#   ISONE_END               - ISO-NE end datetime (YYYY-MM-DDTHH:MM:SSZ)
#   ISONE_MARKET            - Market run identifier (DA or RT)
#   AURUM_KAFKA_BOOTSTRAP_SERVERS - Kafka brokers
#   AURUM_SCHEMA_REGISTRY_URL     - Schema Registry endpoint
#
# Optional environment variables:
#   ISONE.CONF_KEY_SERIALIZER - Kafka key serializer (string/json/avro, default: string)
#   ISONE.CONF_KEY_FORMAT - Kafka key format (json/avro, default: json)
#
#   ISONE_TOPIC             - Kafka topic (default aurum.iso.isone.lmp.v1)
#   ISONE_SUBJECT           - Schema Registry subject (default <topic>-value)
#   ISO_LMP_SCHEMA          - Avro schema JSON populated by run_job.sh
#   ISO_LOCATION_REGISTRY   - Path to iso_nodes.csv (default config/iso_nodes.csv)
#   ISONE_USERNAME          - Basic auth username (if required)
#   ISONE_PASSWORD          - Basic auth password (if required)
#   ISONE_AUTH_HEADER       - Override Authorization header (takes precedence over username/password)
#   ISONE_NODE_FIELD        - Field containing node name (default name)
#   ISONE_NODE_ID_FIELD     - Field containing node id (default ptid)
#   ISONE_NODE_TYPE_FIELD   - Field containing node type (default locationType)
#
# SeaTunnel fetches ISO-NE JSON, projects required fields, and publishes Avro records.

env {
  job.mode = "BATCH"
  parallelism = 1
}

source {
  Http {
    url = "${ISONE_URL}"
    method = "GET"
    params {
      start = "${ISONE_START}"
      end = "${ISONE_END}"
      market = "${ISONE_MARKET}"
    }
    connection_timeout_ms = 20000
    retry = 10
    format = "json"
    schema = { fields { begin = string end = string startTime = string endTime = string start_time = string end_time = string ${ISONE_NODE_FIELD} = string ${ISONE_NODE_ID_FIELD} = string ${ISONE_NODE_TYPE_FIELD} = string lmp = string energy = string congestion = string loss = string } }
    jsonpath = "$$.items[*]"
    headers {
      Authorization = "${ISONE_AUTH_HEADER}"
    }
    auth {
      type = "BASIC"
      username = "${ISONE_USERNAME}"
      password = "${ISONE_PASSWORD}"
      enabled = ${ISONE_HTTP_AUTH_ENABLED}
    }
    result_table_name = "isone_raw"
  }
  LocalFile {
    path = "${ISO_LOCATION_REGISTRY}"
    format = "csv"
    csv {
      delimiter = ","
      header = true
    }
    schema {
      fields {
        iso = string
        location_id = string
        location_name = string
        location_type = string
        zone = string
        hub = string
        timezone = string
      }
    }
    result_table_name = "iso_registry"
  }
}

transform {
  Sql {
    source_table_name = "isone_raw"
    result_table_name = "isone_normalized"
    query = """
      WITH prepared AS (
        SELECT
          COALESCE(begin, startTime, start_time) AS begin_raw,
          COALESCE(end, endTime, end_time) AS end_raw,
          ${ISONE_NODE_FIELD} AS node_name,
          ${ISONE_NODE_ID_FIELD} AS node_id,
          ${ISONE_NODE_TYPE_FIELD} AS node_type,
          CAST(lmp AS DOUBLE) AS price_total,
          CAST(energy AS DOUBLE) AS price_energy,
          CAST(congestion AS DOUBLE) AS price_congestion,
          CAST(loss AS DOUBLE) AS price_loss
        FROM isone_raw
      ), converted AS (
        SELECT
          begin_raw,
          end_raw,
          node_name,
          node_id,
          node_type,
          price_total,
          price_energy,
          price_congestion,
          price_loss,
          CAST(UNIX_TIMESTAMP(SUBSTRING(begin_raw, 1, 19), 'yyyy-MM-dd''T''HH:mm:ss') * 1000000 AS BIGINT) AS interval_start,
          CAST(UNIX_TIMESTAMP(SUBSTRING(end_raw, 1, 19), 'yyyy-MM-dd''T''HH:mm:ss') * 1000000 AS BIGINT) AS interval_end
        FROM prepared
      )
      SELECT
        'ISONE' AS iso_code,
        CASE
          WHEN upper('${ISONE_MARKET}') LIKE 'DA%' THEN 'DAY_AHEAD'
          ELSE 'REAL_TIME'
        END AS market,
        DATEDIFF(TO_DATE(SUBSTRING(c.begin_raw, 1, 10)), TO_DATE('1970-01-01')) AS delivery_date,
        interval_start,
        interval_end,
        CAST((interval_end - interval_start) / 60000000 AS INT) AS interval_minutes,
        CAST(c.node_id AS STRING) AS location_id,
        COALESCE(reg.location_name, CAST(c.node_name AS STRING)) AS location_name,
        COALESCE(reg.location_type, COALESCE(UPPER(CAST(c.node_type AS STRING)), 'NODE')) AS location_type,
        reg.zone AS zone,
        reg.hub AS hub,
        reg.timezone AS timezone,
        COALESCE(c.price_total, 0.0) AS price_total,
        c.price_energy AS price_energy,
        c.price_congestion AS price_congestion,
        c.price_loss AS price_loss,
        'USD' AS currency,
        'MWh' AS uom,
        CAST(c.node_name AS STRING) AS settlement_point,
        NULL AS source_run_id,
        CAST(UNIX_TIMESTAMP() * 1000000 AS BIGINT) AS ingest_ts,
        SHA2(CONCAT_WS('|', c.begin_raw, CAST(c.node_id AS STRING), CAST(c.price_total AS STRING)), 256) AS record_hash,
        MAP('source_timezone', 'America/New_York', 'normalized_to_utc', 'true') AS metadata
      FROM converted c
      LEFT JOIN iso_registry reg
        ON upper(reg.iso) = 'ISONE'
       AND upper(reg.location_id) = upper(CAST(c.node_id AS STRING))
      WHERE interval_start IS NOT NULL
    """
  }
}

sink {
  Kafka {
    plugin_input = "isone_normalized"
    bootstrap.servers = "${AURUM_KAFKA_BOOTSTRAP_SERVERS}"
    topic = "${ISONE_TOPIC}"
    semantic = "AT_LEAST_ONCE"
    format = "avro"
    avro {
      use.schema.registry = true
      schema.registry.url = "${AURUM_SCHEMA_REGISTRY_URL}"
      value.schema.subject = "${ISONE_SUBJECT}"
      value.schema = """${ISO_LMP_SCHEMA}"""
      # Key configuration for message routing and partitioning
      key.serializer = "${ISONE.CONF_KEY_SERIALIZER:-string}"
      key.format = "${ISONE.CONF_KEY_FORMAT:-json}"
    }
    producer {
      request.timeout.ms = 30000
      delivery.timeout.ms = 120000
      retry.backoff.max.ms = 10000
      retry.backoff.ms = 100
      acks = "all"
      enable.idempotence = true
      max.in.flight.requests.per.connection = 5
      linger.ms = 500
      batch.size = 32768
      retries = 5
    }
  }
}
