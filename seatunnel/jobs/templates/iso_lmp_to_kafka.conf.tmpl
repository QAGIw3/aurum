# Generic ISO LMP â†’ Kafka (Avro)
#
# Required environment variables:
#   ISO_BASE_URL             - ISO web service base URL
#   ISO_LMP_ENDPOINT         - LMP endpoint path
#   ISO_START_DATE           - Start date (YYYY-MM-DD)
#   ISO_END_DATE             - End date (YYYY-MM-DD)
#   ISO_MARKET               - Market type (DAM, RTM, etc.)
#   ISO_FORMAT               - Response format (json, csv, xml)
#   AURUM_KAFKA_BOOTSTRAP_SERVERS - Kafka brokers
#   AURUM_SCHEMA_REGISTRY_URL     - Schema Registry endpoint
#
# Optional environment variables:
#   ISO_AUTH_PARAMS          - Additional auth parameters
#   ISO_LMP_TOPIC            - Kafka topic (default aurum.iso.<iso>.lmp.v1)
#   ISO_LMP_SUBJECT          - Schema Registry subject
#   ISO_LMP_SCHEMA           - Avro schema JSON populated by run_job.sh
#   ISO_LOCATION_REGISTRY    - Path to iso_nodes.csv (default config/iso_nodes.csv)
#   ISO_KEY_SERIALIZER       - Kafka key serializer (default: string)
#   ISO_KEY_FORMAT           - Kafka key format (default: json)
#
# Generic template for ISO LMP data ingestion with registry enrichment.

env {
  parallelism = 1
  job.mode = "BATCH"
}

source {
  Http {
    url = "${ISO_BASE_URL}/${ISO_LMP_ENDPOINT}"
    method = "GET"
    params {
      startdate = "${ISO_START_DATE}"
      enddate = "${ISO_END_DATE}"
      market = "${ISO_MARKET}"
      ${ISO_AUTH_PARAMS}
    }
    connection_timeout_ms = 30000
    retry = 3
    retry_backoff_multiplier_ms = 2000
    retry_backoff_max_ms = 60000
    format = "${ISO_FORMAT}"
    rate_limit_sleep_ms = 250
    retry_strategy = "exponential_backoff"
    schema = {
      fields {
        timestamp = string
        node_id = string
        node_name = string
        zone = string
        market = string
        lmp = double
        congestion = double
        losses = double
        source = string
        data_type = string
      }
    }
    result_table_name = "iso_lmp"
  }
  LocalFile {
    path = "${ISO_LOCATION_REGISTRY}"
    format = "csv"
    csv {
      delimiter = ","
      header = true
    }
    schema {
      fields {
        iso = string
        location_id = string
        location_name = string
        location_type = string
        zone = string
        hub = string
        timezone = string
      }
    }
    result_table_name = "iso_registry"
  }
}

transform {
  # Normalize timestamps to UTC and enrich with registry data
  Sql {
    sql = """
      SELECT
        CAST(l.timestamp AS TIMESTAMP) as timestamp,
        l.node_id,
        COALESCE(l.node_name, r.location_name) as node_name,
        COALESCE(l.zone, r.zone) as zone,
        COALESCE(l.market, '${ISO_MARKET}') as market,
        CAST(l.lmp AS DOUBLE) as lmp,
        CAST(l.congestion AS DOUBLE) as congestion,
        CAST(l.losses AS DOUBLE) as losses,
        COALESCE(l.source, '${ISO}') as source,
        COALESCE(l.data_type, 'LMP') as data_type,
        r.location_type,
        r.hub,
        r.timezone,
        NOW() as ingested_at
      FROM iso_lmp l
      LEFT JOIN iso_registry r
        ON upper(r.iso) = upper('${ISO}')
        AND upper(r.location_id) = upper(l.node_id)
      WHERE l.lmp IS NOT NULL AND l.lmp > 0
    """
    result_table_name = "iso_lmp_normalized"
  }

  # Add data quality checks
  Sql {
    sql = """
      SELECT * FROM iso_lmp_normalized
      WHERE timestamp >= '${ISO_START_DATE}T00:00:00'
        AND timestamp <= '${ISO_END_DATE}T23:59:59'
    """
    result_table_name = "iso_lmp_filtered"
  }
}

sink {
  Kafka {
    bootstrap.servers = "${AURUM_KAFKA_BOOTSTRAP_SERVERS}"
    topic = "${ISO_LMP_TOPIC}"
    producer {
      acks = "all"
      enable.idempotence = true
      max.in.flight.requests.per.connection = 5
      linger.ms = 500
      batch.size = 32768
      retries = 10
      retry.backoff.ms = 100
      retry.backoff.max.ms = 10000
      delivery.timeout.ms = 120000
      request.timeout.ms = 30000
    }
    avro {
      use.schema.registry = true
      schema.registry.url = "${AURUM_SCHEMA_REGISTRY_URL}"
      value.schema.subject = "${ISO_LMP_SUBJECT}"
      value.schema = """${ISO_LMP_SCHEMA}"""
      # Key configuration for message routing and partitioning
      key.serializer = "${ISO_KEY_SERIALIZER:-string}"
      key.format = "${ISO_KEY_FORMAT:-json}"
    }
  }
}
