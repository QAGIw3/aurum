# Drought vector event topic â†’ Iceberg (environment.vector_events)
#
# Required environment variables:
#   KAFKA_BOOTSTRAP_SERVERS  - Kafka brokers
#   SCHEMA_REGISTRY_URL      - Schema Registry endpoint
#   ICEBERG_CATALOG_NAME     - Iceberg catalog name (e.g. nessie)
#   ICEBERG_CATALOG_TYPE     - Catalog type (nessie, hive, rest)
#   ICEBERG_URI              - Catalog endpoint / Hive metastore URI
#   ICEBERG_WAREHOUSE        - Warehouse path

env {
  job.mode = "STREAMING"
  checkpoint.interval = 60000
}

source {
  Kafka {
    bootstrap.servers = "${KAFKA_BOOTSTRAP_SERVERS}"
    topic = "aurum.drought.vector_event.v1"
    format = "avro"
    start_mode = "latest"
    avro {
      schema.registry.url = "${SCHEMA_REGISTRY_URL}"
    }
    result_table_name = "vector_raw"
  }
}

transform {
  Sql {
    source_table_name = "vector_raw"
    result_table_name = "vector_enriched"
    query = """
      SELECT
        tenant_id,
        schema_version,
        TO_TIMESTAMP(ingest_ts / 1000000.0) AS ingest_ts,
        ingest_job_id,
        layer,
        event_id,
        region_type,
        region_id,
        CASE WHEN valid_start IS NOT NULL THEN TO_TIMESTAMP(valid_start / 1000000.0) ELSE NULL END AS valid_start,
        CASE WHEN valid_end IS NOT NULL THEN TO_TIMESTAMP(valid_end / 1000000.0) ELSE NULL END AS valid_end,
        value,
        unit,
        category,
        severity,
        source_url,
        geometry_wkt,
        CAST(properties AS STRING) AS properties
      FROM vector_raw
    """
  }
}

sink {
  Iceberg {
    plugin_input = "vector_enriched"
    catalog_name = "${ICEBERG_CATALOG_NAME}"
    catalog_type = "${ICEBERG_CATALOG_TYPE}"
    uri = "${ICEBERG_URI}"
    warehouse = "${ICEBERG_WAREHOUSE}"
    database = "environment"
    table = "vector_events"
    write.distribution-mode = "hash"
    format = "parquet"
    primary_keys = ["layer", "event_id", "ingest_ts"]
    upsert = false
  }
}

