# MISO market reports (DA/RT) â†’ Kafka (Avro)
#
# Required environment variables:
#   MISO_URL                - HTTPS URL for the CSV report (see MISO Market Reports)
#   MISO_MARKET             - Market run (DAY_AHEAD or REAL_TIME)
#   AURUM_KAFKA_BOOTSTRAP_SERVERS - Kafka brokers
#   AURUM_SCHEMA_REGISTRY_URL     - Schema Registry endpoint
#
# Optional environment variables:
#   MISO_TOPIC              - Kafka topic (default aurum.iso.miso.lmp.v1)
#   MISO_SUBJECT            - Schema Registry subject (default <topic>-value)
#   ISO_LMP_SCHEMA          - Avro schema JSON populated by run_job.sh
#   MISO_TIME_FORMAT        - Java datetime pattern for parsing the timestamp column (default yyyy-MM-dd HH:mm:ss)
#   MISO_TIME_COLUMN        - Column containing the interval start timestamp (default Time)
#   MISO_NODE_COLUMN        - Column containing the node/settlement location (default CPNode)
#   MISO_NODE_ID_COLUMN     - Column containing the node id (default "CPNode ID")
#   MISO_LMP_COLUMN         - Column containing the total LMP (default LMP)
#   MISO_CONGESTION_COLUMN  - Column containing congestion (default MCC)
#   MISO_LOSS_COLUMN        - Column containing loss (default MLC)
#   MISO_INTERVAL_SECONDS   - Interval length in seconds (default 3600 for DA, 300 for RT)
#   MISO_CURRENCY           - Currency code (default USD)
#   MISO_UOM                - Price unit (default MWh)
#   ISO_LOCATION_REGISTRY   - Path to iso_nodes.csv (default config/iso_nodes.csv)
#
# SeaTunnel fetches the CSV, normalizes columns, and publishes Avro-encoded records.

env {
  job.mode = "BATCH"
  parallelism = 1
}

source {
  Http {
    url = "dry_run_default"
    method = "GET"
    connection_timeout_ms = 20000
    retry = 5
    retry_backoff_multiplier_ms = 1500
    retry_backoff_max_ms = 90000
    rate_limit_sleep_ms = 200
    retry { interval_ms = 8000, max_retries = 10 }
    format = "csv"
    csv {
      delimiter = ","
      header = true
    }
    result_table_name = "miso_raw"
  }
  LocalFile {
    path = "dry_run_default"
    format = "csv"
    csv {
      delimiter = ","
      header = true
    }
    schema {
      fields {
        iso = string
        location_id = string
        location_name = string
        location_type = string
        zone = string
        hub = string
        timezone = string
      }
    }
    result_table_name = "iso_registry"
  }
}

transform {
  Sql {
    source_table_name = "miso_raw"
    result_table_name = "miso_normalized"
    query = """
      WITH enriched AS (
        SELECT
          `dry_run_default` AS interval_ts,
          `dry_run_default` AS location_name,
          `dry_run_default` AS location_id,
          `dry_run_default` AS price_total_raw,
          `dry_run_default` AS price_congestion_raw,
          `dry_run_default` AS price_loss_raw
        FROM miso_raw
      )
      SELECT
        'MISO' AS iso_code,
        UPPER('dry_run_default') AS market,
        DATEDIFF(TO_DATE(e.interval_ts), TO_DATE('1970-01-01')) AS delivery_date,
        CAST(UNIX_TIMESTAMP(e.interval_ts, 'dry_run_default') * 1000000 AS BIGINT) AS interval_start,
        CAST(UNIX_TIMESTAMP(e.interval_ts, 'dry_run_default') * 1000000
             + dry_run_default * 1000000 AS BIGINT) AS interval_end,
        dry_run_default / 60 AS interval_minutes,
        CAST(e.location_id AS STRING) AS location_id,
        COALESCE(reg.location_name, CAST(e.location_name AS STRING)) AS location_name,
        COALESCE(reg.location_type, 'NODE') AS location_type,
        reg.zone AS zone,
        reg.hub AS hub,
        reg.timezone AS timezone,
        CAST(e.price_total_raw AS DOUBLE) AS price_total,
        CAST(e.price_total_raw AS DOUBLE)
          - COALESCE(CAST(e.price_congestion_raw AS DOUBLE), 0)
          - COALESCE(CAST(e.price_loss_raw AS DOUBLE), 0) AS price_energy,
        CAST(e.price_congestion_raw AS DOUBLE) AS price_congestion,
        CAST(e.price_loss_raw AS DOUBLE) AS price_loss,
        'dry_run_default' AS currency,
        'dry_run_default' AS uom,
        CAST(e.location_name AS STRING) AS settlement_point,
        NULL AS source_run_id,
        CAST(UNIX_TIMESTAMP() * 1000000 AS BIGINT) AS ingest_ts,
        SHA2(CONCAT_WS('|', e.interval_ts, CAST(e.location_id AS STRING), CAST(e.price_total_raw AS STRING)), 256) AS record_hash,
        NULL AS metadata
      FROM enriched e
      LEFT JOIN iso_registry reg
        ON upper(reg.iso) = 'MISO'
       AND upper(reg.location_id) = upper(CAST(e.location_id AS STRING))
      WHERE e.interval_ts IS NOT NULL AND e.price_total_raw IS NOT NULL
    """
  }
}

sink {
  Kafka {
    plugin_input = "miso_normalized"
    bootstrap.servers = "localhost:9092"
    topic = "dry_run_default"
    semantic = "AT_LEAST_ONCE"
    format = "avro"
    avro {
      use.schema.registry = true
      schema.registry.url = "http://localhost:8081"
      value.schema.subject = "dry_run_default"
      value.schema = """{"type":"record","name":"IsoLmpRecord","namespace":"aurum.iso","fields":[{"name":"iso_code","type":"string"}]}"""
    }
    producer {
      request.timeout.ms = 30000
      delivery.timeout.ms = 120000
      retry.backoff.max.ms = 10000
      retry.backoff.ms = 100
      acks = "all"
      enable.idempotence = true
      max.in.flight.requests.per.connection = 5
      linger.ms = 500
      batch.size = 32768
      retries = 5
    }
  }
}
