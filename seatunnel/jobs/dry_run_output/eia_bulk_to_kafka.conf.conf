# EIA bulk CSV archives â†’ Kafka (Avro)
#
# Required environment variables:
#   EIA_BULK_URL             - HTTPS URL to the EIA bulk ZIP/CSV archive
#   EIA_BULK_TOPIC           - Kafka topic to emit Avro records
#   AURUM_KAFKA_BOOTSTRAP_SERVERS  - Kafka brokers (host:port)
#   AURUM_SCHEMA_REGISTRY_URL      - Confluent Schema Registry endpoint
#   EIA_BULK_FREQUENCY       - Frequency label (ANNUAL, QUARTERLY, MONTHLY, WEEKLY, DAILY, HOURLY)
#
# Optional environment variables:
#   EIA_BULK_SERIES_ID_EXPR  - SQL expression for the series identifier (default series_id)
#   EIA_BULK_PERIOD_EXPR     - SQL expression yielding the period token (default period)
#   EIA_BULK_VALUE_EXPR      - SQL expression for the numeric value (default value)
#   EIA_BULK_RAW_VALUE_EXPR  - SQL expression for the raw string value (default value)
#   EIA_BULK_UNITS_EXPR      - SQL expression for the units column (default units)
#   EIA_BULK_AREA_EXPR       - SQL expression for area metadata (default area)
#   EIA_BULK_SECTOR_EXPR     - SQL expression for sector metadata (default sector)
#   EIA_BULK_DESCRIPTION_EXPR- SQL expression for description metadata (default description)
#   EIA_BULK_SOURCE_EXPR     - SQL expression for source attribution (default source)
#   EIA_BULK_DATASET_EXPR    - SQL expression for dataset identifier (default dataset)
#   EIA_BULK_METADATA_EXPR   - SQL expression returning a JSON string/map (default NULL)
#   EIA_BULK_FILTER_EXPR     - Additional SQL predicate appended to WHERE clause (default TRUE)
#   EIA_BULK_SUBJECT         - Schema Registry subject (default <topic>-value)
#   EIA_BULK_SCHEMA          - Avro schema JSON to register (defaults to eia.series.v1)
#   EIA_BULK_DECODE          - Compression codec for the download (default zip)
#   EIA_BULK_CSV_DELIMITER   - CSV field delimiter (default ,)
#   EIA_BULK_SKIP_HEADER     - Header rows to skip (default 1)
#   EIA_BULK_SCHEMA_FIELDS   - SeaTunnel schema fields block (override default columns)

env {
  job.mode = "BATCH"
  parallelism = 1
}

source {
  HttpFile {
    urls = ["dry_run_default"]
    decode = "dry_run_default"
    format = "csv"
    csv {
      field_delimiter = "dry_run_default"
      skip_header_row_number = dry_run_default
    }
    schema = {
      fields {
dry_run_default
      }
    }
    result_table_name = "eia_bulk_raw"
  }
}

transform {
  Sql {
    source_table_name = "eia_bulk_raw"
    result_table_name = "eia_bulk_normalized"
    query = """
      SELECT
        dry_run_default                                 AS series_id,
        dry_run_default                                    AS period,
        CASE UPPER('dry_run_default')
          WHEN 'ANNUAL' THEN CAST(UNIX_TIMESTAMP(CONCAT(dry_run_default, '-01-01'), 'yyyy-MM-dd') * 1000000 AS BIGINT)
          WHEN 'QUARTERLY' THEN CAST(UNIX_TIMESTAMP(concat(substr(dry_run_default, 1, 4), '-', CASE RIGHT(dry_run_default, 2)
              WHEN 'Q1' THEN '01'
              WHEN 'Q2' THEN '04'
              WHEN 'Q3' THEN '07'
              WHEN 'Q4' THEN '10'
              ELSE '01'
            END, '-01'), 'yyyy-MM-dd') * 1000000 AS BIGINT)
          WHEN 'MONTHLY' THEN CAST(UNIX_TIMESTAMP(CONCAT(dry_run_default, '-01'), 'yyyy-MM-dd') * 1000000 AS BIGINT)
          WHEN 'WEEKLY' THEN CAST(UNIX_TIMESTAMP(to_timestamp(dry_run_default, 'yyyy-MM-dd')) * 1000000 AS BIGINT)
          WHEN 'DAILY' THEN CAST(UNIX_TIMESTAMP(to_timestamp(dry_run_default, 'yyyy-MM-dd')) * 1000000 AS BIGINT)
          WHEN 'HOURLY' THEN CAST(UNIX_TIMESTAMP(SUBSTR(dry_run_default, 1, 19), 'yyyy-MM-dd''T''HH:mm:ss') * 1000000 AS BIGINT)
          ELSE CAST(NULL AS BIGINT)
        END                                                        AS period_start,
        CASE UPPER('dry_run_default')
          WHEN 'ANNUAL' THEN CAST(UNIX_TIMESTAMP(add_months(to_timestamp(CONCAT(dry_run_default, '-01-01'), 'yyyy-MM-dd'), 12)) * 1000000 AS BIGINT)
          WHEN 'QUARTERLY' THEN CAST(UNIX_TIMESTAMP(add_months(to_timestamp(concat(substr(dry_run_default, 1, 4), '-', CASE RIGHT(dry_run_default, 2)
              WHEN 'Q1' THEN '01'
              WHEN 'Q2' THEN '04'
              WHEN 'Q3' THEN '07'
              WHEN 'Q4' THEN '10'
              ELSE '01'
            END, '-01'), 'yyyy-MM-dd'), 3)) * 1000000 AS BIGINT)
          WHEN 'MONTHLY' THEN CAST(UNIX_TIMESTAMP(add_months(to_timestamp(CONCAT(dry_run_default, '-01'), 'yyyy-MM-dd'), 1)) * 1000000 AS BIGINT)
          WHEN 'WEEKLY' THEN CAST(UNIX_TIMESTAMP(to_timestamp(dry_run_default, 'yyyy-MM-dd') + INTERVAL 7 DAYS) * 1000000 AS BIGINT)
          WHEN 'DAILY' THEN CAST(UNIX_TIMESTAMP(to_timestamp(dry_run_default, 'yyyy-MM-dd') + INTERVAL 1 DAY) * 1000000 AS BIGINT)
          WHEN 'HOURLY' THEN CAST((UNIX_TIMESTAMP(SUBSTR(dry_run_default, 1, 19), 'yyyy-MM-dd''T''HH:mm:ss') + 3600) * 1000000 AS BIGINT)
          ELSE CAST(NULL AS BIGINT)
        END                                                        AS period_end,
        'dry_run_default'                                    AS frequency,
        CASE
          WHEN dry_run_default IS NULL OR dry_run_default = '' THEN NULL
          ELSE CAST(dry_run_default AS DOUBLE)
        END                                                        AS value,
        CAST(dry_run_default AS STRING)                 AS raw_value,
        dry_run_default                                     AS unit,
        dry_run_default                                      AS area,
        dry_run_default                                    AS sector,
        NULL                                                       AS seasonal_adjustment,
        dry_run_default                               AS description,
        dry_run_default                                    AS source,
        dry_run_default                                   AS dataset,
        dry_run_default                                  AS metadata,
        CAST(UNIX_TIMESTAMP() * 1000000 AS BIGINT)                 AS ingest_ts
      FROM eia_bulk_raw
      WHERE (dry_run_default)
    """
  }
}

sink {
  Kafka {
    plugin_input = "eia_bulk_normalized"
    bootstrap.servers = "localhost:9092"
    topic = "dry_run_default"
    semantic = "AT_LEAST_ONCE"
    format = "avro"
    avro {
      use.schema.registry = true
      schema.registry.url = "http://localhost:8081"
      value.schema.subject = "dry_run_default"
      value.schema = """dry_run_default"""
    }
    producer {
      acks = "all"
      enable.idempotence = true
      max.in.flight.requests.per.connection = 5
      linger.ms = 500
      batch.size = 32768
      retries = 5
    }
  }
}
