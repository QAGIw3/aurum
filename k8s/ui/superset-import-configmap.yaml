apiVersion: v1
kind: ConfigMap
metadata:
  name: superset-import
  namespace: aurum-dev
data:
  import_iso_dashboard.py: |
    import json
    import os
    from superset.app import create_app
    from superset import db
    from superset.models.dashboard import Dashboard
    from superset.models.slice import Slice
    from superset.connectors.sqla.models import SqlaTable

    DATASETS = [
      ("public", "iso_lmp_last_24h"),
      ("public", "iso_lmp_hourly"),
      ("public", "iso_lmp_daily"),
      ("public", "iso_lmp_negative_7d"),
    ]

    app = create_app()
    with app.app_context():
      # Find Timescale DB
      from superset.models.core import Database
      dbobj = db.session.query(Database).filter_by(database_name=os.environ.get("SUPERSET_TIMESCALE_NAME", "Timescale")).first()
      if not dbobj:
        print("Timescale database not found in Superset; skipping dashboard import")
        raise SystemExit(0)

      # Ensure datasets exist and are synced
      datasets = {}
      for schema, table in DATASETS:
        ds = db.session.query(SqlaTable).filter_by(database_id=dbobj.id, schema=schema, table_name=table).first()
        if not ds:
          ds = SqlaTable(database=dbobj, schema=schema, table_name=table)
          db.session.add(ds)
          db.session.commit()
        try:
          ds.fetch_metadata()
          db.session.commit()
        except Exception as exc:  # pragma: no cover
          print(f"Warning: failed to fetch metadata for {schema}.{table}: {exc}")
        datasets[(schema, table)] = ds

      # Create basic table charts
      charts = []
      def upsert_table_chart(name: str, ds: SqlaTable, columns: list[str]):
        params = {
          "viz_type": "table",
          "time_range": "No filter",
          "all_columns": columns,
          "adhoc_filters": [],
          "server_page_length": 50,
          "order_by_cols": ["[\"interval_start\",+false]"] if "interval_start" in columns else [],
        }
        slc = db.session.query(Slice).filter_by(slice_name=name).first()
        if not slc:
          slc = Slice(
            slice_name=name,
            viz_type="table",
            datasource_id=ds.id,
            datasource_type="table",
            params=json.dumps(params),
          )
          db.session.add(slc)
          db.session.commit()
        charts.append(slc)

      upsert_table_chart("ISO LMP - Last 24h", datasets[("public", "iso_lmp_last_24h")], [
        "interval_start", "iso_code", "location_id", "market", "price_total"
      ])
      upsert_table_chart("ISO LMP - Hourly Avg", datasets[("public", "iso_lmp_hourly")], [
        "interval_start", "iso_code", "location_id", "market", "price_avg", "sample_count"
      ])
      upsert_table_chart("ISO LMP - Daily Avg", datasets[("public", "iso_lmp_daily")], [
        "interval_start", "iso_code", "location_id", "market", "price_avg", "sample_count"
      ])
      upsert_table_chart("ISO LMP - Negative (7d)", datasets[("public", "iso_lmp_negative_7d")], [
        "interval_start", "iso_code", "location_id", "market", "price_total"
      ])

      # Create dashboard and add charts
      title = os.environ.get("SUPERSET_ISO_DASHBOARD", "ISO LMP Ops")
      dash = db.session.query(Dashboard).filter_by(dashboard_title=title).first()
      if not dash:
        dash = Dashboard(dashboard_title=title)
        db.session.add(dash)
        db.session.commit()

      for slc in charts:
        if slc not in dash.slices:
          dash.slices.append(slc)
      db.session.commit()

      print(f"Created/updated dashboard '{dash.dashboard_title}' with {len(charts)} charts")

